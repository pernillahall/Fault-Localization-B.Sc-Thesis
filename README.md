
# AI-Driven Fault Localization in Large-Scale Software Systems

This repository contains the full implementation from the bachelor thesis:  
**"AI-Driven Fault Localization in Large-Scale Software Systems"**  

ğŸ“„ **[Thesis Report PDF](./Thesis_Report.pdf)**

---

## Overview

To maintain software quality, efficient fault localization is essential, and the historical
data in bug-tracking systems offers opportunities for AI-assisted debugging. This thesis investigates whether
machine learning can assist software developers in identifying likely fault locations using only the textual
content of bug reports. Each resolved bug report is linked to its corresponding code fixes, after which models are 
trained to output a ranked list of likely fault locations. Natural language preprocessing, feature extraction, 
and data augmentation are explored to improve performance, which is measured using Top-k Accuracy, Recall@k, MAP
and MRR.

---

## Dataset
The data used in the study is derived from a software project at ABB Robotics, VÃ¤sterÃ¥s, Sweden, which due to confidential reasons is not published.
This repository serves as a replication package, enabling this research to be replicated in any software setting where resolved bug reports and their corresponding
code fixes are available.

---

## Models Compared

| Type              | Techniques                                          |
|-------------------|-----------------------------------------------------|
| **Traditional ML**| Logistic Regression, Support Vector Machine (SVM), Random Forest |
| **LLMs**          | RoBERTa-Base, DistilRoBERTa (fine-tuned)            |
| **Features**      | TF-IDF, Sentence-BERT (all-mpnet-base-v2)           |

---

## Repository Structure

```bash
.
â”œâ”€â”€ data/                                 # Processed data and label-binarized splits for training/evaluation (excluded due to confidentiality)
â”‚   â””â”€â”€ prepared_data/                    # Includes train/val/test splits and label binarization artifacts
â”‚       â”œâ”€â”€ train_df.csv
â”‚       â”œâ”€â”€ val_df.csv
â”‚       â”œâ”€â”€ test_df.csv
â”‚       â”œâ”€â”€ y_train_bin.npy, ...
â”‚       â”œâ”€â”€ mlb.joblib, ...
â”‚
â”œâ”€â”€ llms/                                 # Locally stored pre-trained transformer models
â”‚   â”œâ”€â”€ all-mpnet-base-v2-local-files/    # For sentence embeddings
â”‚   â”œâ”€â”€ distil-roberta-base-local-files/  # Smaller RoBERTa model
â”‚   â””â”€â”€ roberta-base-local-files/         # Full-size RoBERTa model
â”‚
â”œâ”€â”€ results/                              # Output directory for model checkpoints, logs, and evaluation results (excluded)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                           # Model training, hyperparameter tuning, and evaluation
â”‚   â”‚   â”œâ”€â”€ eval_utils.py                 # Shared functions for evaluation metrics 
â”‚   â”‚   â”œâ”€â”€ final_evaluation.py           # Runs final benchmark on all saved models
â”‚   â”‚   â”œâ”€â”€ focal_loss_trainer.py         # Custom trainer using focal loss for LLM fine-tuning
â”‚   â”‚   â”œâ”€â”€ grid_search_lr_*.py           # Grid search for Logistic Regression with TF-IDF or SBERT features
â”‚   â”‚   â”œâ”€â”€ grid_search_rf_*.py           # Grid search for Random Forest with TF-IDF or SBERT
â”‚   â”‚   â”œâ”€â”€ grid_search_svm_*.py          # Grid search for SVM with TF-IDF or SBERT
â”‚   â”‚   â””â”€â”€ roberta_hyper_search.py       # Optuna-based hyperparameter tuning for RoBERTa models
â”‚   
â”‚   â””â”€â”€ preprocessing/                    # Scripts for loading, processing, and splitting bug data
â”‚       â”œâ”€â”€ extract_bugs.py               # Extracts bug reports and linked commits from Azure DevOps
â”‚       â”œâ”€â”€ map_paths_to_folders.py       # Maps file paths in commits to subfolder/component labels
â”‚       â”œâ”€â”€ text_preprocess.py            # Tokenization, stopword removal, decamelcasing, lemmatization
â”‚       â”œâ”€â”€ split_data.py                 # Stratified train/val/test split
â”‚       â”œâ”€â”€ data_augmentation.py          # Augments samples with synonym/random swap techniques
â”‚       â””â”€â”€ get_stats.py                  # Utility to compute path frequencies
â”‚
â”œâ”€â”€ requirements.txt                      # Python dependencies 
â”œâ”€â”€ .env                                  # Azure DevOps credentials (excluded)
â”œâ”€â”€ .gitignore                            # Ignored files/folders
â””â”€â”€ README.md                             # This file
```

---

## Environment

The experiments were conducted on a machine with the following specifications:

- **CPU:** 12th Gen Intel(R) Core(TM) i9-12950HX  
- **RAM:** 32.0 GB  
- **GPU:** NVIDIA RTX A3000 (12.0 GB VRAM)  
- **Operating System:** Windows 11  
- **Python Version:** 3.10

---

## Getting Started

âš ï¸ **Note:** Dataset and model artifacts are excluded due to confidentiality reasons.

### 1. Install dependencies

Before running the code, install the required Python packages by running the following command in your terminal or command prompt:

```bash
pip install numpy pandas scikit-learn matplotlib seaborn joblib sentence-transformers datasets optuna nltk azure-devops msrest python-dotenv beautifulsoup4 scikit-multilearn
````

For torch, check the appropriate CUDA version for your GPU and install accordingly from: https://pytorch.org/get-started/locally/

### 2. Azure DevOps Setup (for `extract_bugs.py`)

Create an `.env` file with:

```env
personal_access_token=your_token
organization_url=https://dev.azure.com/your_org
team_path=YourTeam
team=YourTeamName
project=YourProject
```

### 3. Data Preprocessing Pipeline

#### Step 1: Extract bug reports from Azure DevOps using ```src/preprocessing/extract_bugs.py```

#### Step 2: Count file occurences using ```src/preprocessing/get_stats.py```. Manually annotate the resulting csv file with a 'Label' column.
  
#### Step 3: Map file paths to folder labels (targets) with ```src/preprocessing/map_paths_to_folders.py```

#### Step 4: Clean, decamelcase, remove stop words, lemmatize text using ```src/preprocessing/text_preprocess.py```

#### Step 5: Stratified train/val/test split + label binarization through ```src/preprocessing/split_data.py```

#### Step 6 (Optional): Augment underrepresented samples with ```src/preprocessing/data_augmentation.py```

---

### Model Training & Benchmarking

#### Train and tune traditional ML models (LR, SVM, RF) with GridSearch:
```
- src/models/grid_search_lr_tfidf.py
- src/models/grid_search_lr_sbert.py
- src/models/grid_search_rf_tfidf.py
- src/models/grid_search_rf_sbert.py
- src/models/grid_search_svm_tfidf.py
- src/models/grid_search_svm_sbert.py
```

#### Fine-tune RoBERTa / DistilRoBERTa:
```
- src/models/roberta_hyper_search.py
```

#### Run final benchmark on all models:
```
- src/models/final_evaluation.py
```

---

## Citation

```bibtex
@bachelorsthesis{hall2025faultlocalization,
  title={AI-Driven Fault Localization in Large-Scale Software Systems},
  author={Hall, Pernilla},
  year={2025},
  school={MÃ¤lardalen University}
}
```
